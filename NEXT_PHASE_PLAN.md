# ParLeap - Next Phase Implementation Plan

**Last Updated:** December 22, 2025  
**Current Status:** Phase 2.4 Complete ‚úÖ | Phase 3 Complete ‚úÖ | Phase 3.4 Complete ‚úÖ | READY FOR DEPLOYMENT üöÄ  
**Next Focus:** Production Deployment (Railway + Vercel) ‚Üí Phase 4 (ML Optimization)

### üü¢ System Status
- ‚úÖ Backend implementation complete (Phase 2.4 + Phase 3)
- ‚úÖ Frontend implementation complete (Phase 3.4)
- ‚úÖ All tests passing (13/13)
- ‚úÖ Zero TypeScript errors
- ‚úÖ Zero linter errors
- ‚úÖ All documentation complete
- ‚úÖ Ready for deployment to production

---

## üéØ Strategic Overview

We've completed the foundation and latency monitoring infrastructure. Now we need to build the **core AI functionality** that makes ParLeap work:

1. **Data Layer** (Supabase) - Real content management
2. **Audio Input** (Frontend) - Capture and stream audio
3. **AI Processing** (STT) - Convert speech to text
4. **Matching Engine** - Match transcription to slides

---

## üìã Phase 1.2: Supabase Integration (Foundation)

**Priority:** HIGH - Needed for real data before we can test matching

### Goals
- Set up Supabase project
- Create database schema
- Initialize Supabase clients
- Replace mock data with real queries

### Tasks

#### 1.2.1 Supabase Project Setup
- [x] Create Supabase project at https://supabase.com
- [x] Get project URL and API keys
- [x] Configure environment variables:
  - `NEXT_PUBLIC_SUPABASE_URL`
  - `NEXT_PUBLIC_SUPABASE_ANON_KEY`
  - `SUPABASE_SERVICE_ROLE_KEY` (backend only)

#### 1.2.2 Database Schema Migration
- [x] Run migration: `supabase/migrations/001_initial_schema.sql`
- [x] Verify tables created:
  - `profiles`
  - `songs`
  - `events`
  - `event_items`
- [x] Set up Row Level Security (RLS) policies
- [x] Test RLS policies with test user

#### 1.2.3 Supabase Client Initialization
- [x] Frontend: Updated `frontend/lib/supabase/client.ts` ‚úÖ
- [x] Backend: Created `backend/src/config/supabase.ts` ‚úÖ
- [x] Test connection from both frontend and backend
- [x] Verify environment variables are set correctly

#### 1.2.4 Replace Mock Data
- [x] Backend: Created `backend/src/services/eventService.ts` ‚úÖ
- [x] Updated `handleStartSession` to fetch from Supabase ‚úÖ
  - Queries `events` table for event data
  - Queries `event_items` for setlist
  - Queries `songs` for lyrics
- [x] Parse lyrics into lines (automatic in eventService) ‚úÖ
- [x] Cache in memory (SessionState) ‚úÖ
- [x] Error handling for missing data ‚úÖ

**Estimated Time:** 2-3 hours  
**Dependencies:** None  
**Blockers:** None

---

## üìã Phase 2.3: Audio Capture (Frontend)

**Priority:** HIGH - Core functionality for real-time processing

### Goals
- Capture audio from browser microphone
- Stream audio chunks to WebSocket
- Provide visual feedback to operator
- Handle permissions and errors gracefully

### Tasks

#### 2.3.1 Browser Microphone Access
- [x] Created `useAudioCapture` hook (`frontend/lib/hooks/useAudioCapture.ts`) ‚úÖ
- [x] Request microphone permissions ‚úÖ
- [x] Handle permission denied/blocked states ‚úÖ
- [x] Display permission request UI ‚úÖ
- [x] Error handling for unsupported browsers ‚úÖ

#### 2.3.2 MediaRecorder Setup
- [x] Initialize `MediaRecorder` API ‚úÖ
- [x] Configure audio format:
  - Sample rate: 16000 Hz (optimal for STT) ‚úÖ
  - Channels: 1 (mono) ‚úÖ
  - Encoding: WebM Opus ‚úÖ
- [x] Set up chunk size (1000ms chunks) ‚úÖ
- [x] Handle `dataavailable` events ‚úÖ

#### 2.3.3 Audio Streaming to WebSocket
- [x] Convert audio chunks to Base64 ‚úÖ
- [x] Send via `AUDIO_DATA` message type ‚úÖ
- [x] Include audio metadata (sample rate, format) ‚úÖ
- [x] Handle WebSocket disconnection during streaming ‚úÖ
- [x] Queue chunks if WebSocket is temporarily disconnected ‚úÖ

#### 2.3.4 Visual Feedback Components
- [x] Created `AudioLevelMeter` component ‚úÖ
  - Visual waveform with animated bars ‚úÖ
  - Real-time audio level visualization ‚úÖ
- [x] Created `MicrophoneStatus` component ‚úÖ
  - Shows recording/stopped state ‚úÖ
  - Displays permission status ‚úÖ
  - Error messages ‚úÖ
- [x] Added to operator dashboard (WebSocketTest component) ‚úÖ

#### 2.3.5 Integration with WebSocket
- [x] Start audio capture when session starts ‚úÖ
- [x] Stop audio capture when session ends ‚úÖ
- [x] Pause/resume functionality ‚úÖ
- [x] Cleanup on component unmount ‚úÖ

**Estimated Time:** 4-6 hours  
**Dependencies:** Phase 2.1 (WebSocket Protocol) ‚úÖ  
**Blockers:** None

---

## üìã Phase 2.4: AI Transcription Integration (STT Provider)

**Priority:** HIGH - Core AI functionality

### Goals
- Choose and integrate STT provider
- Stream audio to STT service
- Receive real-time transcriptions
- Handle errors and retries

### Tasks

#### 2.4.1 STT Provider Selection
- [ ] Evaluate options:
  - **Google Cloud Speech-to-Text**
    - Pros: High accuracy, streaming API, good latency
    - Cons: Requires GCP account, pricing
  - **ElevenLabs Scribe**
    - Pros: Simple API, good for music/lyrics
    - Cons: Newer service, less proven
- [ ] Make decision based on:
  - Latency requirements (<500ms)
  - Accuracy for music/lyrics
  - Cost
  - Ease of integration
- [ ] Set up provider account and get API keys

#### 2.4.2 Backend STT Integration
- [ ] Install provider SDK (e.g., `@google-cloud/speech`)
- [ ] Create `backend/src/services/stt.ts` service
- [ ] Initialize streaming client
- [ ] Configure audio format matching frontend
- [ ] Handle streaming audio chunks
- [ ] Parse transcription results
- [ ] Extract confidence scores

#### 2.4.3 WebSocket Audio Handler
- [ ] Update `handleAudioData` in `backend/src/websocket/handler.ts`
- [ ] Convert Base64 audio to buffer
- [ ] Forward to STT service
- [ ] Receive transcription results
- [ ] Send `TRANSCRIPT_UPDATE` messages with:
  - `text`: Transcription text
  - `isFinal`: Whether transcription is final
  - `confidence`: Confidence score (0-1)
  - `timing`: Timing metadata

#### 2.4.4 Error Handling & Retry Logic
- [ ] Handle STT service errors
- [ ] Implement exponential backoff retry
- [ ] Fallback to manual override if STT fails
- [ ] Logging for debugging
- [ ] User-friendly error messages

**Estimated Time:** 6-8 hours  
**Dependencies:** Phase 2.3 (Audio Capture)  
**Blockers:** STT provider account setup

---

## üìã Phase 2.5: Backend Audio Processing Pipeline

**Priority:** MEDIUM - Optimization and refinement

### Goals
- Optimize audio buffer management
- Maintain rolling transcription buffer
- Prepare for fuzzy matching

### Tasks

#### 2.5.1 Audio Buffer Management
- [ ] Implement audio chunk queue
- [ ] Handle out-of-order chunks
- [ ] Buffer management for network jitter
- [ ] Memory cleanup

#### 2.5.2 Rolling Transcription Buffer
- [ ] Maintain last 5 seconds of transcription
- [ ] Update buffer with new transcriptions
- [ ] Handle final vs. interim transcriptions
- [ ] Prepare buffer for matching algorithm

#### 2.5.3 Logging & Monitoring
- [ ] Add structured logging
- [ ] Track transcription latency
- [ ] Monitor STT service health
- [ ] Alert on high error rates

**Estimated Time:** 2-3 hours  
**Dependencies:** Phase 2.4 (STT Integration)  
**Blockers:** None

---

## üìã Phase 3: Predictive Matching Algorithm

**Priority:** HIGH - Core value proposition

### Goals
- Match transcribed text to song lyrics
- Auto-advance slides when match found
- Handle edge cases and errors

### Tasks

#### 3.1 Content Loading (Already Partially Done)
- [x] Cache setlist in memory ‚úÖ
- [x] Cache setlist in browser ‚úÖ
- [ ] Fetch from Supabase (Phase 1.2)
- [ ] Parse lyrics into lines
- [ ] Handle updates during session

#### 3.2 Fuzzy Matching Engine
- [ ] Install `string-similarity` (already in package.json)
- [ ] Create `backend/src/services/matcher.ts`
- [ ] Implement matching algorithm:
  - Compare rolling buffer to current song lines
  - Use similarity threshold: 0.85
  - Handle partial matches
- [ ] Performance optimization:
  - Only match against current song
  - Cache similarity calculations
  - Early exit on high confidence matches
- [ ] Match confidence scoring

#### 3.3 Slide Management Logic
- [x] Track current slide index ‚úÖ
- [ ] Detect match for current line
- [ ] Auto-advance to next slide on match
- [ ] Detect last line match ‚Üí advance to next song
- [ ] Handle manual overrides (already done ‚úÖ)
- [ ] Queue management for smooth transitions

**Estimated Time:** 6-8 hours  
**Dependencies:** Phase 1.2 (Supabase), Phase 2.4 (STT)  
**Blockers:** STT integration needed for real transcriptions

---

## üéØ Recommended Implementation Order

### Sprint 1: Data Foundation (Week 1)
1. **Supabase Integration** (Phase 1.2)
   - Set up project and schema
   - Replace mock data
   - Test with real events/songs

**Deliverable:** Real data flowing through system

---

### Sprint 2: Audio Input (Week 1-2)
2. **Audio Capture** (Phase 2.3)
   - Browser microphone access
   - Audio streaming to WebSocket
   - Visual feedback components

**Deliverable:** Audio streaming from browser to backend

---

### Sprint 3: AI Processing (Week 2)
3. **STT Integration** (Phase 2.4)
   - Choose provider
   - Integrate streaming STT
   - Real-time transcription

**Deliverable:** Live transcription working

---

### Sprint 4: Matching Engine (Week 2-3)
4. **Fuzzy Matching** (Phase 3.2)
   - Implement matching algorithm
   - Auto-advance logic
   - Performance optimization

**Deliverable:** End-to-end: Audio ‚Üí Transcription ‚Üí Match ‚Üí Slide Change

---

## üìä Success Metrics

### Phase 1.2 (Supabase)
- ‚úÖ Can create events and songs in Supabase
- ‚úÖ Backend fetches real data on START_SESSION
- ‚úÖ Frontend displays real event/song data

### Phase 2.3 (Audio Capture)
- ‚úÖ Microphone permission granted
- ‚úÖ Audio chunks streaming to backend
- ‚úÖ Visual feedback working
- ‚úÖ Latency: <100ms from capture to send

### Phase 2.4 (STT)
- ‚úÖ Transcriptions arriving in real-time
- ‚úÖ Confidence scores included
- ‚úÖ Latency: <300ms from audio to transcription
- ‚úÖ Error rate: <5%

### Phase 3 (Matching)
- ‚úÖ Matches detected with >85% similarity
- ‚úÖ Slides auto-advance correctly
- ‚úÖ Total latency: <500ms end-to-end
- ‚úÖ Accuracy: >90% correct matches

---

## üö® Risks & Mitigations

### Risk 1: STT Provider Latency Too High
- **Mitigation:** Test both Google and ElevenLabs, choose faster one
- **Fallback:** Manual override always available

### Risk 2: Browser Audio Permissions
- **Mitigation:** Clear UI for permission requests, handle gracefully
- **Fallback:** Manual override

### Risk 3: Matching Accuracy
- **Mitigation:** Tune similarity threshold, test with real lyrics
- **Fallback:** Operator can manually correct

### Risk 4: Network Issues During Live Event
- **Mitigation:** Already implemented (RTT monitoring, slide caching)
- **Fallback:** Offline mode with cached slides

---

## üìù Notes

- **Start with Supabase** - Real data is needed for meaningful testing
- **Audio capture can be tested independently** - Use mock STT initially
- **STT integration is critical path** - Blocks matching algorithm testing
- **Matching algorithm can be tested with mock transcriptions** - Don't wait for STT

---

## üéØ Immediate Next Steps

1. **Today:** Set up Supabase project and run migrations
2. **This Week:** Complete Supabase integration and audio capture
3. **Next Week:** Integrate STT provider and build matching engine

**Ready to start?** Begin with Phase 1.2 (Supabase Integration)!

---

## üöÄ Post-Launch Roadmap

### Phase 7: Content Import Integration

**Priority:** High (Post-Launch)  
**Timeline:** 2-3 weeks after MVP stabilization

#### CCLI SongSelect API Integration

**Strategic Value:**
- Major UX improvement - eliminates manual lyric entry
- Competitive advantage - industry-standard integration
- Solves current stanza parsing challenges automatically
- Aligns with target market (churches using CCLI)

**Implementation Phases:**

1. **CCLI Developer Partner Application**
   - Apply for CCLI Developer Partner program
   - Get API credentials and documentation
   - Review API rate limits and terms
   - Set up OAuth application in CCLI portal

2. **OAuth Integration**
   - Implement OAuth 2.0 flow
   - Store tokens securely (Supabase encrypted column)
   - Handle token refresh
   - Add "Connect CCLI" button to Songs Library

3. **Search & Import UI**
   - Build search interface
   - Display search results
   - Import song with formatted lyrics
   - Handle errors gracefully

4. **Testing & Polish**
   - Test with real CCLI accounts
   - Verify stanza formatting
   - Test error scenarios
   - Performance optimization

**User Flow:**
```
User ‚Üí Songs Library ‚Üí "Import from CCLI" ‚Üí OAuth ‚Üí Search ‚Üí Import ‚Üí Auto-formatted lyrics saved
```

**Technical Components:**
- OAuth integration (`frontend/lib/auth/ccli.ts`)
- API service (`frontend/lib/services/ccliApi.ts`)
- UI components (`frontend/components/songs/CcliImport.tsx`)
- Database migration (add `ccli_oauth_token` to profiles)
- Server actions (`importCcliSong`, `searchCcliSongs`)

**See:** `CCLI_SONGSELECT_INTEGRATION.md` for complete specification

---

### Phase 8: Smart Bible Listen (Cost Optimization)

**Priority:** Medium (Post-Launch)  
**Timeline:** 3-4 weeks after MVP stabilization  
**Status:** üìã Documented - Ready for Implementation

#### Smart Bible Listen Feature

**Strategic Value:**
- Significant cost reduction for Bible mode (87-93% STT savings)
- Enables longer sermon support without API cost concerns
- Maintains accuracy while optimizing resource usage
- Competitive advantage for cost-conscious users

**Problem Statement:**
- Current Bible mode runs STT continuously (expensive for 40+ min sermons)
- User concern: "I don't want my AI token to get finished by listening to all words"
- Need to only activate STT when Bible content detected

**Solution Architecture:**
Two-stage hybrid system:
1. **Stage 1:** Local wake-word detection (zero API cost)
   - Pattern matching on audio chunks in browser
   - Detects Bible book names, wake phrases ("chapter", "verse", "it is written")
   - Maintains 10-second audio ring buffer
2. **Stage 2:** Selective STT window (controlled cost)
   - Activates ElevenLabs STT only when wake word detected
   - 30-second window after trigger
   - Auto-shutoff if no Bible reference found
   - Extends window if reference detected (follow mode)

**Implementation Phases:**

1. **Frontend Wake-Word Detection**
   - Create `useBibleWakeWord` hook
   - Implement pattern matching for wake words
   - Add audio ring buffer to `useAudioCapture`
   - Test wake word detection accuracy
   - Add UI toggle in Operator HUD

2. **Backend Selective STT**
   - Add `smartListenEnabled` flag to SessionState
   - Implement conditional STT initialization
   - Add 30-second window timer
   - Implement auto-shutoff logic
   - Test STT activation/deactivation

3. **Integration & Testing**
   - End-to-end testing with real sermons
   - Verify cost savings calculations
   - Test edge cases (rapid references, long pauses)
   - Performance optimization
   - UI polish (cost indicator, toggles)

4. **Quote Matching (Optional)**
   - Create `bibleQuoteMatcher` service
   - Implement full-text search across all verses
   - Add fuzzy matching for partial quotes
   - Add UI toggle for quote search (opt-in)
   - Test with various quote formats

**Cost Savings:**
- **40-minute sermon example:**
  - Current: 2,400 seconds of STT
  - With Smart Listen: ~300 seconds (5-10 references √ó 30s)
  - **Savings: 87-93% cost reduction**

**User Preferences:**
- Smart Listen enabled by default when Bible mode is on
- 30-second STT window after trigger (configurable)
- Quote-only matching: Opt-in only (disabled by default)
- Cost savings indicator in UI

**Technical Components:**
- Frontend: `frontend/lib/hooks/useBibleWakeWord.ts` (new)
- Frontend: `frontend/lib/hooks/useAudioCapture.ts` (modify)
- Backend: `backend/src/websocket/handler.ts` (modify)
- Backend: `backend/src/services/bibleQuoteMatcher.ts` (new, optional)
- UI: `frontend/components/operator/OperatorHUD.tsx` (modify)

**See:** `SMART_BIBLE_LISTEN.md` for complete specification

