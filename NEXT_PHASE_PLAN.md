# ParLeap - Next Phase Implementation Plan

**Last Updated:** December 13, 2025  
**Current Status:** Phase 2.1 & 2.2 Complete ‚úÖ  
**Next Focus:** Phase 1.2 (Supabase) ‚Üí Phase 2.3 (Audio Capture) ‚Üí Phase 2.4 (STT Integration)

---

## üéØ Strategic Overview

We've completed the foundation and latency monitoring infrastructure. Now we need to build the **core AI functionality** that makes ParLeap work:

1. **Data Layer** (Supabase) - Real content management
2. **Audio Input** (Frontend) - Capture and stream audio
3. **AI Processing** (STT) - Convert speech to text
4. **Matching Engine** - Match transcription to slides

---

## üìã Phase 1.2: Supabase Integration (Foundation)

**Priority:** HIGH - Needed for real data before we can test matching

### Goals
- Set up Supabase project
- Create database schema
- Initialize Supabase clients
- Replace mock data with real queries

### Tasks

#### 1.2.1 Supabase Project Setup
- [ ] Create Supabase project at https://supabase.com
- [ ] Get project URL and API keys
- [ ] Configure environment variables:
  - `NEXT_PUBLIC_SUPABASE_URL`
  - `NEXT_PUBLIC_SUPABASE_ANON_KEY`
  - `SUPABASE_SERVICE_ROLE_KEY` (backend only)

#### 1.2.2 Database Schema Migration
- [ ] Run migration: `supabase/migrations/001_initial_schema.sql`
- [ ] Verify tables created:
  - `profiles`
  - `songs`
  - `events`
  - `event_items`
- [ ] Set up Row Level Security (RLS) policies
- [ ] Test RLS policies with test user

#### 1.2.3 Supabase Client Initialization
- [ ] Frontend: Update `frontend/lib/supabase/client.ts` (already exists)
- [ ] Backend: Create `backend/src/lib/supabase.ts`
- [ ] Test connection from both frontend and backend
- [ ] Verify environment variables are set correctly

#### 1.2.4 Replace Mock Data
- [ ] Backend: Update `handleStartSession` to fetch from Supabase
  - Query `events` table for event data
  - Query `event_items` for setlist
  - Query `songs` for lyrics
- [ ] Parse lyrics into lines (split by newline or custom delimiter)
- [ ] Cache in memory (already implemented)
- [ ] Error handling for missing data

**Estimated Time:** 2-3 hours  
**Dependencies:** None  
**Blockers:** None

---

## üìã Phase 2.3: Audio Capture (Frontend)

**Priority:** HIGH - Core functionality for real-time processing

### Goals
- Capture audio from browser microphone
- Stream audio chunks to WebSocket
- Provide visual feedback to operator
- Handle permissions and errors gracefully

### Tasks

#### 2.3.1 Browser Microphone Access
- [ ] Create `AudioCapture` hook (`frontend/lib/hooks/useAudioCapture.ts`)
- [ ] Request microphone permissions
- [ ] Handle permission denied/blocked states
- [ ] Display permission request UI
- [ ] Error handling for unsupported browsers

#### 2.3.2 MediaRecorder Setup
- [ ] Initialize `MediaRecorder` API
- [ ] Configure audio format:
  - Sample rate: 16000 Hz (optimal for STT)
  - Channels: 1 (mono)
  - Encoding: WebM Opus or PCM
- [ ] Set up chunk size (e.g., 1000ms chunks)
- [ ] Handle `dataavailable` events

#### 2.3.3 Audio Streaming to WebSocket
- [ ] Convert audio chunks to Base64
- [ ] Send via `AUDIO_DATA` message type
- [ ] Include audio metadata (sample rate, format)
- [ ] Handle WebSocket disconnection during streaming
- [ ] Queue chunks if WebSocket is temporarily disconnected

#### 2.3.4 Visual Feedback Components
- [ ] Create `AudioLevelMeter` component
  - Visual waveform or level bars
  - Real-time audio level visualization
- [ ] Create `MicrophoneStatus` component
  - Shows recording/stopped state
  - Displays permission status
  - Error messages
- [ ] Add to operator dashboard

#### 2.3.5 Integration with WebSocket
- [ ] Start audio capture when session starts
- [ ] Stop audio capture when session ends
- [ ] Pause/resume functionality
- [ ] Cleanup on component unmount

**Estimated Time:** 4-6 hours  
**Dependencies:** Phase 2.1 (WebSocket Protocol) ‚úÖ  
**Blockers:** None

---

## üìã Phase 2.4: AI Transcription Integration (STT Provider)

**Priority:** HIGH - Core AI functionality

### Goals
- Choose and integrate STT provider
- Stream audio to STT service
- Receive real-time transcriptions
- Handle errors and retries

### Tasks

#### 2.4.1 STT Provider Selection
- [ ] Evaluate options:
  - **Google Cloud Speech-to-Text**
    - Pros: High accuracy, streaming API, good latency
    - Cons: Requires GCP account, pricing
  - **ElevenLabs Scribe**
    - Pros: Simple API, good for music/lyrics
    - Cons: Newer service, less proven
- [ ] Make decision based on:
  - Latency requirements (<500ms)
  - Accuracy for music/lyrics
  - Cost
  - Ease of integration
- [ ] Set up provider account and get API keys

#### 2.4.2 Backend STT Integration
- [ ] Install provider SDK (e.g., `@google-cloud/speech`)
- [ ] Create `backend/src/services/stt.ts` service
- [ ] Initialize streaming client
- [ ] Configure audio format matching frontend
- [ ] Handle streaming audio chunks
- [ ] Parse transcription results
- [ ] Extract confidence scores

#### 2.4.3 WebSocket Audio Handler
- [ ] Update `handleAudioData` in `backend/src/websocket/handler.ts`
- [ ] Convert Base64 audio to buffer
- [ ] Forward to STT service
- [ ] Receive transcription results
- [ ] Send `TRANSCRIPT_UPDATE` messages with:
  - `text`: Transcription text
  - `isFinal`: Whether transcription is final
  - `confidence`: Confidence score (0-1)
  - `timing`: Timing metadata

#### 2.4.4 Error Handling & Retry Logic
- [ ] Handle STT service errors
- [ ] Implement exponential backoff retry
- [ ] Fallback to manual override if STT fails
- [ ] Logging for debugging
- [ ] User-friendly error messages

**Estimated Time:** 6-8 hours  
**Dependencies:** Phase 2.3 (Audio Capture)  
**Blockers:** STT provider account setup

---

## üìã Phase 2.5: Backend Audio Processing Pipeline

**Priority:** MEDIUM - Optimization and refinement

### Goals
- Optimize audio buffer management
- Maintain rolling transcription buffer
- Prepare for fuzzy matching

### Tasks

#### 2.5.1 Audio Buffer Management
- [ ] Implement audio chunk queue
- [ ] Handle out-of-order chunks
- [ ] Buffer management for network jitter
- [ ] Memory cleanup

#### 2.5.2 Rolling Transcription Buffer
- [ ] Maintain last 5 seconds of transcription
- [ ] Update buffer with new transcriptions
- [ ] Handle final vs. interim transcriptions
- [ ] Prepare buffer for matching algorithm

#### 2.5.3 Logging & Monitoring
- [ ] Add structured logging
- [ ] Track transcription latency
- [ ] Monitor STT service health
- [ ] Alert on high error rates

**Estimated Time:** 2-3 hours  
**Dependencies:** Phase 2.4 (STT Integration)  
**Blockers:** None

---

## üìã Phase 3: Predictive Matching Algorithm

**Priority:** HIGH - Core value proposition

### Goals
- Match transcribed text to song lyrics
- Auto-advance slides when match found
- Handle edge cases and errors

### Tasks

#### 3.1 Content Loading (Already Partially Done)
- [x] Cache setlist in memory ‚úÖ
- [x] Cache setlist in browser ‚úÖ
- [ ] Fetch from Supabase (Phase 1.2)
- [ ] Parse lyrics into lines
- [ ] Handle updates during session

#### 3.2 Fuzzy Matching Engine
- [ ] Install `string-similarity` (already in package.json)
- [ ] Create `backend/src/services/matcher.ts`
- [ ] Implement matching algorithm:
  - Compare rolling buffer to current song lines
  - Use similarity threshold: 0.85
  - Handle partial matches
- [ ] Performance optimization:
  - Only match against current song
  - Cache similarity calculations
  - Early exit on high confidence matches
- [ ] Match confidence scoring

#### 3.3 Slide Management Logic
- [x] Track current slide index ‚úÖ
- [ ] Detect match for current line
- [ ] Auto-advance to next slide on match
- [ ] Detect last line match ‚Üí advance to next song
- [ ] Handle manual overrides (already done ‚úÖ)
- [ ] Queue management for smooth transitions

**Estimated Time:** 6-8 hours  
**Dependencies:** Phase 1.2 (Supabase), Phase 2.4 (STT)  
**Blockers:** STT integration needed for real transcriptions

---

## üéØ Recommended Implementation Order

### Sprint 1: Data Foundation (Week 1)
1. **Supabase Integration** (Phase 1.2)
   - Set up project and schema
   - Replace mock data
   - Test with real events/songs

**Deliverable:** Real data flowing through system

---

### Sprint 2: Audio Input (Week 1-2)
2. **Audio Capture** (Phase 2.3)
   - Browser microphone access
   - Audio streaming to WebSocket
   - Visual feedback components

**Deliverable:** Audio streaming from browser to backend

---

### Sprint 3: AI Processing (Week 2)
3. **STT Integration** (Phase 2.4)
   - Choose provider
   - Integrate streaming STT
   - Real-time transcription

**Deliverable:** Live transcription working

---

### Sprint 4: Matching Engine (Week 2-3)
4. **Fuzzy Matching** (Phase 3.2)
   - Implement matching algorithm
   - Auto-advance logic
   - Performance optimization

**Deliverable:** End-to-end: Audio ‚Üí Transcription ‚Üí Match ‚Üí Slide Change

---

## üìä Success Metrics

### Phase 1.2 (Supabase)
- ‚úÖ Can create events and songs in Supabase
- ‚úÖ Backend fetches real data on START_SESSION
- ‚úÖ Frontend displays real event/song data

### Phase 2.3 (Audio Capture)
- ‚úÖ Microphone permission granted
- ‚úÖ Audio chunks streaming to backend
- ‚úÖ Visual feedback working
- ‚úÖ Latency: <100ms from capture to send

### Phase 2.4 (STT)
- ‚úÖ Transcriptions arriving in real-time
- ‚úÖ Confidence scores included
- ‚úÖ Latency: <300ms from audio to transcription
- ‚úÖ Error rate: <5%

### Phase 3 (Matching)
- ‚úÖ Matches detected with >85% similarity
- ‚úÖ Slides auto-advance correctly
- ‚úÖ Total latency: <500ms end-to-end
- ‚úÖ Accuracy: >90% correct matches

---

## üö® Risks & Mitigations

### Risk 1: STT Provider Latency Too High
- **Mitigation:** Test both Google and ElevenLabs, choose faster one
- **Fallback:** Manual override always available

### Risk 2: Browser Audio Permissions
- **Mitigation:** Clear UI for permission requests, handle gracefully
- **Fallback:** Manual override

### Risk 3: Matching Accuracy
- **Mitigation:** Tune similarity threshold, test with real lyrics
- **Fallback:** Operator can manually correct

### Risk 4: Network Issues During Live Event
- **Mitigation:** Already implemented (RTT monitoring, slide caching)
- **Fallback:** Offline mode with cached slides

---

## üìù Notes

- **Start with Supabase** - Real data is needed for meaningful testing
- **Audio capture can be tested independently** - Use mock STT initially
- **STT integration is critical path** - Blocks matching algorithm testing
- **Matching algorithm can be tested with mock transcriptions** - Don't wait for STT

---

## üéØ Immediate Next Steps

1. **Today:** Set up Supabase project and run migrations
2. **This Week:** Complete Supabase integration and audio capture
3. **Next Week:** Integrate STT provider and build matching engine

**Ready to start?** Begin with Phase 1.2 (Supabase Integration)!

